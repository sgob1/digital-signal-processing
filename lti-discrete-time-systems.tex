\chapter{Linear-Time-Invariant Discrete-time System}

As we have already seen in Section~\ref{sec:ltiSystems}, in system analysis---among other fields of study---a \textbf{Linear Time-Invariant System (LTI System)} is a system that produces an output signal from any input signal subject to the constraints of linearity and time-invariance; these terms are briefly defined below. These properties apply (exactly or ap\-pro\-xi\-ma\-te\-ly) to many important physical systems, in which case the response $y(t)$ of the system to an arbitrary input $x(t)$ can be found directly using convolution: $y(t) = x(t) \circledast h(t)$ where $h(t)$ is called the \emph{system's impulse response} and symbol \textquotesingle$\circledast$\textquotesingle{} represents convolution (not to be confused with multiplication, as is frequently employed by the symbol in computer languages). Wh\-at's more, there are systematic methods for solving any such system (determining $h(t)$), whereas systems not meeting both properties are generally more difficult (or impossible) to solve analytically. A good example of an LTI system is any electrical circuit consisting of resistors, capacitors, inductors and linear amplifiers~\cite{bib:ltiSystems}.

In this chapter we will understand in a deeper sense the properties of an LTI system, the conditions under which it is subject, and in the end study the correlation of sequences relative to various kinds of sequences, be them energy signals or power signals.

\section{Conditions on LTI systems}

\subsection{Stability Condition of an LTI system}

In Section~\ref{sec:BIBOStableSystems} we learnt that \emph{BIBO stable systems} are those under which any bounded input yields a bounded output as well. This is to say that as long as we input a signal with absolute value less than some constant, we are guaranteed to have an output with absolute value less than some other constant.

In this section we will understand the conditions under which any LTI discrete-time system is a BIBO stable system, that is the \textbf{stability condition}. The following theorem yields a fundamental property that BIBO stable LTI systems must follow.

\begin{thm}[BIBO Stability of an LTI discrete-time system]\label{thm:ltiBiboStability}
An LTI discrete-time system is BIBO stable \emph{if and only if} its impulse response sequence $\{h[n]\}$ is \emph{absolutely summable}, that is when the quantity $\mathcal S$ is such that
    \begin{equation}\label{eqn:ltiBiboStability}
        \mathcal S = \sum_{n=-\infty}^\infty \left|h[n]\right| < \infty.
    \end{equation}
\end{thm}

\begin{proof}
    Let the impulse response $h[n]$ being a real sequence. Since the input sequence $x[n]$ is bounded, one has that $|x[n]| \leq B_x < \infty$. Therefore, magnitude of the output will be
    \begin{align}
        |y[n]| &= \left| \sum_{k=-\infty}^\infty h[k]x[n-k]\right| \\
               &\leq \sum_{k=-\infty}^\infty |h[k]||x[n-k]| \\
               &\leq B_x \sum_{k=-\infty}^\infty |h[k]| \\
               & = B_x \mathcal S\label{eqn:ltiBiboStabilityEnd}
    \end{align}
    two quantities that are indeed bounded.

    To prove the inverse relation, the most straightforward way is to simply consider a bounded output $y[n]$, so that \[|y[n]| \leq B_y.\] Let the input signal be
\[
    x[n] = \left\{ \begin{array}{lc} \mbox{sgn} (h[-n]), & \mbox{ if } h[-n] \neq 0 \\ K, & \mbox{ if } h[-n] = 0\end{array}\right.,
\]
where the function $\mbox{sgn}(c) = +1$ if $c > 0$ and $\mbox{sgn}(c) = -1$ in the opposite case of $c<0$, and $|K| \leq 1$. The sequence $x[n]$ is bounded, since for each and every of its samples its value is limited.

For this peculiar input, the output $y[n]$ at $n=0$ will be
\[
    y[0] = \sum_{k=-\infty}^\infty \mbox{sgn}(h[k])h[k] = \mathcal S \leq B_y < \infty,
\]
but since $|y[n]| \leq B_y$ this implies that $\mathcal S < \infty$.
\end{proof}

Equation~\ref{eqn:ltiBiboStabilityEnd} shows that the output for an LTI discrete-time system will be bounded, and in particular it will be the product of two bounds---the first related to the input signal $x[n]$, and the second associated to the absolute sum of the impulse response.

Some special LTI discrete-time systems are BIBO stable and are worthy of spending some time on them. Consider, for example, the system with such impulse response \[h[n] = \alpha^n \mu[n].\]

For this system it holds that, to be BIBO stable,
\[
    \mathcal S = \sum_{n=-\infty}^\infty |\alpha|^n \mu[n] = \underbrace{\sum_{n=0}^\infty |\alpha|^n =\frac{1}{1-|\alpha|}}_{|\alpha| < 1} .
\]

For the above reason, if $|\alpha| < 1$ then the system is BIBO stable, as $\mathcal S < \infty$. For $|\alpha| \geq 1$ the system is \emph{not} BIBO stable.

\subsection{Causality Condition of an LTI system}

The \textbf{causality condition} for an LTI system is a condition that severely limites the design choices of the impulse response for any causal LTI discrete-time system. Let's first exhamine the behavior of two input sequences which, despite being different sequences, are identical before a certain time instant $n_0$. Indeed, let $x_1$ and $x_2$ be two input sequences that follow the subsequent rules,
\begin{align*}
    x_1[n] = x_2[n] &\mbox{ for } n \leq n_0,\\
    x_1[n] \neq x_2[n] &\mbox{ for } n > n_0,
\end{align*}
then the corresponding outputs at $n=n_0$ of an LTI system with an impulse response $h[n]$ are given by either one of the following formulas,

\begin{align*}
    y_1[n_0] &= \sum_{k=-\infty}^\infty h[k]x_1[n_0 - k]\\
             &= \sum_{k=0}^\infty h[k]x_1[n_0 - k] + \sum_{k=-\infty}^{-1} h[k]x_1[n_0 - k]\\
    y_2[n_0] &= \sum_{k=-\infty}^\infty h[k]x_2[n_0 - k]\\
             &= \sum_{k=0}^\infty h[k]x_2[n_0 - k] + \sum_{k=-\infty}^{-1} h[k]x_2[n_0 - k].
\end{align*}

If the LTI system is also a causal system, then it must be true that
\[ y_1[n_0] = y_2[n_0] \]
as the behavior of the two input sequences is identical up to the sample $n_0$. But this means that
\[
    \sum_{k=0}^\infty h[k] x_1[n_0 - k] = \sum_{k=0}^\infty h[k]x_2[n_0 - k],
\]
and also it must be true that
\[
    \sum_{k=-\infty}^{-1} h[k] x_1[n_0 - k] = \sum_{k=-\infty}^{-1} h[k]x_2[n_0 - k].
\]

Now, since $x_1[n] \neq x_2[n], n> n_0$ the only way the condition
\[
    \sum_{k=-\infty}^{-1} h[k] x_1[n_0 - k] = \sum_{k=-\infty}^{-1} h[k]x_2[n_0 - k]
\]
is satisfied is when both sums are equal to zero---a condition that is indeed satisfied if and only if
\begin{equation}\label{eqn:causalityConditionLTI}
    h[k] \equiv 0, \forall k < 0.
\end{equation}

Equation~\ref{eqn:causalityConditionLTI} poses a strict condition on the impulse response $h[n]$ of a causal LTI system: in order for an LTI discrete-time system to be \emph{causal}, its impulse response $\{h[n]\}$ must be a \emph{causal} sequence as well.

As an example, consider the discrete-time system
\[
    y[n] = \alpha_1 x[n] + \alpha_2 x[n-1] + \alpha_3 x[n-2] + \alpha_4 x[n-3],
\]
which will possess an impulse response of the form
\[
    \{h[n]\} = \begin{Bmatrix}\underset{\uparrow}{\alpha_1} & \alpha_2 & \alpha_3 & \alpha_4\end{Bmatrix},
\]
a doubtlessly causal sequence.

Another example is certainly the discrete-time accumulator, defined by~\ref{eqn:accumulatorEquation},
\[
    y[n] = \sum_{l=-\infty}^{n} x[l].
\]

The accumulator is a \emph{causal} system as it possesses a causal impulse response, provided by the formula
\[
    h[n] = \sum_{l=-\infty}^{n} \delta[i] =\mu[n].
\]

A good counter--example is the factor-of-$2$ interpolator, having equation
\[
    y[n] = x_u[n] + \frac 1 2(x_u[n-1] + x_u[n+1]).
\]

The interpolator is a \emph{noncausal} system, because it has a noncausal impulse response that's given by
\[
    \{h[n]\} = \begin{Bmatrix} 0.5 & \underset{\uparrow}{1} & 0.5\end{Bmatrix}.
\]

As we have previously seen, any noncausal LTI discrete-time system with a finite-length impulse response can be realized as causal systems by inserting appropriate amounts of delay, to push the ``future'' samples back into the past so that there is no term $x[n+k]$ with $k>0$. Adopting this method, the simplest causal version of the above system is the one delayed of a single time unit,
\[
    y[n] = x_u[n-1] + \frac 1 2(x_u[n-2] + x_u[n]).
\]

\section{Finite-Dimensional LTI Discrete-time Systems}
A very important subclass of LTI systems is characterized by a linear constant coefficients difference equation. Those systems must abide to the following form
\begin{equation}\label{eqn:finiteDimensionalLtiSystems}
    \sum_{k=0}^N d_k y[n-k] = \sum_{k=0}^M p_k x[n-k],
\end{equation}
where $x[n]$ and $y[n]$ are, respectively, the input and the output of the LTI system. Sequences $\{d_k\}$ and $\{p_k\}$ are constants characterizing the finite-dimensional system.

Finite-dimensional LTI systems possess an \emph{order}; the order of the system is given by the quantity $\max {(N,M)}$, which is indeed the order of the difference equation. It is possible to implement any LTI systems characterized by a constant coefficient difference equation with two finite sums of products. In fact, if one assumes that the system is a causal LTI finite-dimensional system and that the coefficient $d_0 \neq 0$, then the output $y[n]$ can be obtained by \emph{recursively} computing the output
\begin{equation}\label{eqn:finiteDimensionalLtiSystemsRecursive}
    y[n]= -\sum_{k=1}^N \frac {d_k}{d_0} y[n-k] + \sum_{k=0}^M \frac{p_k}{d_0} x[n-k],
\end{equation}
obtained by rearranging Equation~\ref{eqn:finiteDimensionalLtiSystems}. The output can be obtained this way for all $n \geq n_0$, by knowing both $x[n]$ and all the $N$ initial conditions $y[n_0-1], y[n_0-2],\dots,y[n_0-N]$.

\section{Classification of LTI Discrete-time Systems}

Linear-Time-Invariant discrete-time systems can be classified by some of their properties, such as
\begin{itemize}
    \item by \textbf{length of impulse response}---usually, one wants to deal with finite-length signals, so it's very important to divide impulse responses that might generate infinite-length sequences from those who do not;
    \item by \textbf{output calculation process}---the disctinction is based on the process from which one can compute the output, that is if it is possible to compute the output sequence recursively or not;
    \item by \textbf{coefficients nature}---indeed, coefficients might be real or complex.
\end{itemize}

\subsubsection{Classification based on impulse response length}
\emph{Impulse response length} is a crucial aspect to know when dealing with LTI systems. Like we have already seen, if the impulse response $h[n]$ is of finite length then the convolution with a finite-length input sequence $x[n]$ \emph{will} yield a finite-length output as well. All systems that possess a finite-length impulse response are said to be \textbf{finite impulse response (FIR)} discrete-time systems. FIR systems are those for which
\[
    h[n] = 0 \mbox{ for } n < N_1, \mbox{ and } n > N_2,
\]
with $N_1 < N_2$.

The convolution sum of a FIR system can simply be written as
\[
    y[n]=\sum_{k=N_1}^{N_2} h[k]x[n-k].
\]

The output $y[n]$ of a FIR LTI discrete-time system can be computed directly from the convolution sum, as it is a finite sum of products (and thus we can use the convolution sum). Examples of FIR LTI systems are the $M$-moving average system and the linear interpolators, both of which have a finite-length impulse response.

If the impulse response is of infinite length, the corresponding system is known as an \textbf{infinite impulse response (IIR)}, a concept that it is opposed to the FIR one. The class of the IIR systems are characterized by a linear constant coefficient difference equations, and for this reason it cannot be computed by means of a convolution sum. Examples of IIR systems are the accumulator, which as we already know, is defined by
\[
    y[n] = y[n-1] + x[n]
\]
which is clearly a linear constant coefficient difference equation yielding a recursion system.

\subsubsection{Classification based on output calculation process}
Calculation process plays a determinant role over classification of LTI systems. There are two categories of LTI systems concerning calculation process:
\begin{itemize}
    \item \emph{nonrecursive systems}, that are those whose output can be calculated sequentially, knowing only the present and past input samples;
    \item \emph{recursive systems} are instead all those systems whose output computation in addition to the present and past input samples has to take into account past (some) output samples.
\end{itemize}

\subsubsection{Classification based on coefficients nature}
Even coefficients nature plays a role into sequences classification. As coefficients can be either real or complex, there are two categories:
\begin{itemize}
    \item \emph{real discrete-time systems}---whose impulse response samples are real valued;
    \item \emph{complex discrete-time systems}---whose impulse response samples are complex valued, that is when \emph{at least one} coefficient is complex valued.
\end{itemize}

\section{Correlation of signals}

In general, the \textbf{correlation of two signals} or waveforms is defined as the measure of \emph{similarity} between those signals. There are many applications in which it might be necessary to \emph{compare} one reference signal with one or more signals to determine the similarity between the pair, and ultimately to determine additional information based off the similarity.

To name one, in digital communications a set of data symbols in a predetermined alphabet are represented by a set of unique discrete-time sequences that codify them. If one of these sequences has been transmitted, the receiver has to determined \emph{which particular sequence has been received}, and it does so by \emph{comparing the received signal with every member of possible sequences from the set}. In such an application, the correlation of two sequences comes in handy, as it is a measure of similarity---the more the similar two sequences are, the more probable a received sequence codifies a certain symbol in the alphabet.

Another application in which correlation proves useful is in the radar and sonar applications; the received signal refected from the target is a delayed version of the transmitted signal, and by \emph{measuring the delay} one can soon determine the location of the target. Correlation helps on this as measuring the delay might be brought back as the problem of finding the greatest similarity (correlation) between two sequences, the received sequence and a delayed step sequence $A\mu[n-\theta]$, with $A$ amplitude and $\theta$ delay.§FIXME[prove this claim] The detection problem gets much more complicated in practice---as often the received signal is corrupted by additive, random noise.

The correlation of signals idea leads to the following definition of \textbf{cross-correlation},
\begin{defin}[Cross-correlation]
    Let $x[n]$ and $y[n]$ be two energy signals. The \emph{cross-correlation sequence} $r_{xy}[l]$ is a sequence such that
    \begin{equation}\label{eqn:crossCorrelation}
        r_{xy}[l] = \sum_{n=-\infty}^\infty x[n]y[n-l], l = 0, \pm 1, \pm 2, \dots
    \end{equation}
    where $l$ is a parameter called \emph{lag}, that indicates the relative time-shift between the pair of sequences. Signal $x[n]$ is said to be the \emph{reference sequence}.
\end{defin}

The cross-correlation between signals resembles the convolution sum, except the fact one of the arguments in the sequences is opposite---in this case, one has $n-l$ in place of $l - n$ like in the case of the convolution sum. Cross-correlation is a measure of similarity between a pair of \emph{energy signals}. Indeed, this similarity is much clearer if one rewrites the expression for the cross-correlation as follows,
\begin{align*}
    r_{xy}[l] &= \sum_{n=-\infty}^\infty x[n]y[n-l]\\
              &= \sum_{n=-\infty}^\infty x[n]y[-(l-n)]\\
              &= x[l] \circledast y[-l].
\end{align*}
a rewriting that remarks that the cross-correlation is like performing a convolution with one signal whose arguments are inverted, with $y[-l]$ instead of $y[l]$.


The signal $y[n]$ is said to be shifted by $l$ samples to the \emph{right} with respect to the reference sequence $x[n]$ for positive values of $l$, and on contrary it is said to be shifted by $l$ samples to the \emph{right} for negative values of $l$. This is as expected, as the $l$ is simply a delay--advance parameter very similar to the case of the convolution sum. The ordering of the subscripts $xy$ in the definition of the cross-correlation $r_{xy}[l]$ is important specifies that $x[n]$ is the \emph{reference sequence} around which the sequence $y[n]$ is shifted with respect to.

Of course, signals can be interchanged. The cross-correlation equation would soon become
\begin{align*}
    r_{yx}[l] &= \sum_{n=-\infty}^\infty y[n]x[n-l]\\
              &= \sum_{m=-\infty}^\infty x[m+l]y[m]\\
              &= r_{xy}[-l],
\end{align*}
which highlights that $r_{yx}[l]$ can be obtained by \emph{time-reversing} $r_{xy}[l]$.

What happens now if one performs a cross-correlation between two signals that are ultimately the same? The operation is said to be the \textbf{autocorrelation} and it is given by the following definition,
\begin{defin}[Autocorrelation]
    Let $x[n]$ be an energy signal. The \emph{autocorrelation sequence} of $x[n]$ is given by
    \begin{equation}\label{eqn:autocorrelation}
        r_{xx}[l] = \sum_{n=-\infty}^\infty x[n]x[n-l]
    \end{equation}
    obtained by setting $y[n]=x[n]$ in the definition of the cross-correlation sequence $r_{xy}[l]$.
\end{defin}

A peculiar remark of the autocorrelation is that its value at time instant $0$ is equal to the total energy of the signal $x[n]$. That is,

\begin{predicate}[Equivalence between autocorrelation and energy]
    Let $x[n]$ be an energy signal and $r_{xx}[l]$ be the autocorrelation function of $x$. Then,
    \begin{equation}\label{eqn:equivalenceAutocorrelationEnergy}
        r_{xx}[0] = \mathcal E_x.
    \end{equation}
\end{predicate}

\begin{proof}
    It is enough to substitute
    \[
        r_{xx}[0] = \sum_{n=-\infty}^\infty x[n]x[n-l] =\sum_{n=-\infty}^\infty |x[n]|^2 = \mathcal E_x
    \]
    as the sum is equal to the definition of total energy as in Equation~\ref{eqn:totalEnergy}.
\end{proof}

The above predicate shows that, when the correlation is computed with no delay involved, the total energy is obtained. From the relation $r_{yx}[l] = r_{xy}[-l]$ it immediately follows that $r_{xx}[l] = r_{xx}[-l]$, implying that the autocorrelation is an \emph{even} function for real sequences $x[n]$.

\subsection{Properties of cross-correlation and autocorrelation}
Let $x[n]$ and $y[n]$ be two energy sequences. The energy of the combined sequence $\alpha x[n] + y[n-l]$ is also finite and nonnegative\footnote{Energy is nonnegative because it is squared.}, that is
\begin{align*}
    \sum_{n=-\infty}^\infty (\alpha x[n] + y[n-l])^2 &= \underbrace{\alpha^2 \sum_{n=-\infty}^\infty x^2[n]}_{\mathcal E_x} + \underbrace{2\alpha \sum_{n=-\infty}^\infty x[n] y[n-l]}_{cross-correlation}\\ &+ \underbrace{\sum_{n=-\infty}^\infty y^2[n-l]}_{\mathcal E_y} \geq 0.
\end{align*}

This means that
\[
    \alpha^2 r_{xx}[0] + 2\alpha r_{xy}[l] + r_{yy}[0] \geq 0,
\]
where $r_{xx}[0] = \mathcal E_x > 0$ and $r_{yy}[0] = \mathcal E_y > 0$.

One can rewrite the above equations as the following matrix system,

\[
    \begin{bmatrix} \alpha & 1 \end{bmatrix}\begin{bmatrix} r_{xx}[0] & r_{xy}[l] \\ r_{xy}[l] & r_{yy}[0]\end{bmatrix}\begin{bmatrix}\alpha \\ 1\end{bmatrix} \geq 0,
\]
that is
\[
    \begin{bmatrix} \alpha & 1 \end{bmatrix}\begin{bmatrix} \mathcal E_x & r_{xy}[l] \\ r_{xy}[l] & \mathcal E_y \end{bmatrix}\begin{bmatrix}\alpha \\ 1\end{bmatrix} = \begin{bmatrix} \alpha & 1 \end{bmatrix}\bm{R}\begin{bmatrix} \alpha \\ 1 \end{bmatrix} \geq 0,
\]
for any finite value of $\alpha$.

The matrix $\bm R$ is a \emph{positive semidefinite} matrix. As such, $\det {(\bm R)} \geq 0,$ that is \[\det {(\bm R)} =r_{xx}[0]r_{yy}[0] - r^2_{xy}[l] \geq 0\] or equivalently \begin{equation}\label{eqn:crossCorrelationUpperBound}|r_{xy}| \leq \sqrt{r_{xx}[0]r_{yy}[0]}= \sqrt{\mathcal E_x\mathcal E_y}.\end{equation} The latter specifies that the magnitude of the cross-correlation is always smaller than the geometric average between the energies of signals $x$ and $y$, and provides an upper bound for the cross-correlation samples. In the case of the autocorrelation it soon becomes
\begin{equation}\label{eqn:autocorrelationUpperBound}
    |r_{xx}[l]| \leq r_{xx}[0] = \mathcal E_x,
\end{equation}
a remarkable property of the autocorrelation. In practice, above inequality establishes that the autocorrelation has (at least) a peak in its time instant $0$---or when $x[n]$ is completely superposed on itself---whose value is equal to the total energy ofthe signal $x[n]$. At zero lag $l=0$ the sample value of the autocorrelation sequence has its maximum value, which is equal to the total energy of the sequence.

Consider now the case of \[y[n] = \pm b x[n-N],\] where $N$ is an integer and $b>0$ is an arbitrary number. That case, \[\mathcal E_y = b^2\mathcal E_x,\] since the multiplication constant appears squared after the energy evaluation.

But this means that the geometric mean \[|r_{xy}[l] \leq \sqrt{\mathcal E_x \mathcal E_y} = \sqrt{b^2 \mathcal E^2_x} = b\mathcal E_x\] for a signal scaled by factor $b$. By means of inequality
\[
    |r_{xy}[l]| \leq \sqrt{\mathcal E_x \mathcal E_y}
\]
one obtains
\begin{equation}\label{eqn:crossCorrelationScalingBounds}
- b \mathcal E_x = - br_{xx}[0]  \leq |r_{xy}[l]| \leq b r_{xx}[0] = b \mathcal E_x,
\end{equation}
an important inequality that expresses a precise rule regarding bounds for all signals scaled by a constant factor $b$. By scaling a signal $x[n]$ by a constant factor $b$, that is picking $y[n] = bx[n]$ with $b\in\R\setminus\{0\}$ the bound for the cross-correlation expressed by the total energy increases by a factor of $b$ as well.

Autocorrelation and cross-correlation can both be easily computed by means of the \textsc{Matlab} function \texttt{xcorr}.

\subsection{Normalized forms of correlation}

Correlation possesses so-called \textbf{normalized forms}. Normalized forms are useful to compare correlations with other signals or to obtain a value that is bounded between $-1$ and $+1$.

\begin{defin}[Normalized autocorrelation and cross-correlation]
    Let $x[n]$ and $y[n]$ be energy signals---the \emph{normalized autocorrelation} and \emph{normalized cross-correlation} sequences are given by, respectively,
    \begin{equation}\label{eqn:normalizedCorrelationFormulas}
        \rho_{xx}[l] = \frac{r_{xx}[l]}{r_{xx}[0]}, \rho_{xy}[l]=\frac{r_{xy}[l]}{\sqrt{r_{xx}[0]r_{yy}[0]}}.
    \end{equation}
\end{defin}

Normalized forms are often employed for convenience in comparing and displaying correlation values, as both are smaller than $1$. Indeed,
\begin{align*}
    -1 \leq &\rho_{xx}[l] \leq +1,
    -1 \leq &\rho_{xy}[l] \leq +1,
\end{align*}
independently of the range of values of sequences $x[n]$ and $y[n]$.

\subsection{Computing the correlation for various kinds of signals}

Power signals are all those that, despite being of infinite-length, possess a finite amount of average power as expressed in Equation~\ref{eqn:averagePower}. The cross-correlation for them is defined as in the following definition,
\begin{defin}[Cross-correlation for power signals]
    Let $x[n]$ and $y[n]$ be \emph{power signals}. Then the \emph{cross-correlation} $r_{xy}$ is given by
    \begin{equation}\label{eqn:crossCorrelationPowerSignals}
        r_{xy}[l] = \lim_{K\rightarrow \infty} \frac 1 {2K+1} \sum_{n=-K}^K x[n]y[n-l],
    \end{equation}
    with $\frac 1 {2K+1}$ normalization factor.
\end{defin}

Similarly, one can define the autocorrelation for a power signal as in the following definition,
\begin{defin}[Autocorrelation for power signals]
    Let $x[n]$ be a \emph{power signal}. Then the \emph{autocorrelation} $r_{xx}$ is given by
    \begin{equation}\label{eqn:crossCorrelationPowerSignals}
        r_{xx}[l] = \lim_{K\rightarrow \infty} \frac 1 {2K+1} \sum_{n=-K}^K x[n]x[n-l],
    \end{equation}
    with $\frac 1 {2K+1}$ normalization factor.
\end{defin}

The correlation sequences have a distinct formula for periodic signals as well. Following the footsteps of the previous two definitions, one might want to define cross-correlation and autocorrelation for periodic sequences as well.

\begin{defin}[Cross-correlation for periodic signals]
    Let $x[n]$ and $y[n]$ be periodic signals of period $N$. Then the \emph{cross-correlation} $r_{xy}$ is defined as
    \begin{equation}\label{eqn:crossCorrelationPeriodicSignals}
        r_{xy}[l] = \frac 1 N \sum_{n=0}^{N-1} x[n]y[n-l].
    \end{equation}
\end{defin}

\begin{defin}[Autocorrelation for periodic signals]
    Let $x[n]$ be a periodic signal of period $N$. Then the \emph{autocorrelation} $r_{xx}$ is defined as
    \begin{equation}\label{eqn:crossCorrelationPeriodicSignals}
        r_{xx}[l] = \frac 1 N \sum_{n=0}^{N-1} x[n]x[n-l].
    \end{equation}
\end{defin}

Both quantities, as expected, are evaluated in a single period of the involved sequences. They will produce periodic signals, therefore both cross-correlation $r_{xy}[l]$ and $r_{xx}[l]$ are \emph{periodic signals of period $N$}. The periodicity property of the autocorrelation sequence can be exploited to determine the period of any periodic signal that might be affected by an additive random noise. Periodic signals affected by random noise will lose their periodicity, as the impredictable and random nature of the noise will inevitably lead to a non-periodic signal.

Indeed, it is sufficient to evaluate the autocorrelation for a disturbed signal $w[n] = x[n] + d[n]$, with $x$ original signal and $d$ additive noise, and evaluate the autocorrelation for signal $w[n]$. If $w$ is composed of $M-1$ samples (that is, we collected $M$ samples of $w$, from $0 \leq n \leq M-1$) and $M \ll N$, it is possible to capture the period of $x$ by computing the autocorrelation of $w[n]$. In fact,
\begin{align*}
    r_{ww}[l] &= \frac 1 M \sum_{n=0}^{M-1} w[n]w[n-l]\\
              &= \frac 1 M \sum_{n=0}^{M-1} (x[n] + d[n])(x[n-l] + d[n-l])\\
              &= \frac 1 M \sum_{n=0}^{M-1} x[n] x[n-l] + \frac 1 M \sum_{n=0}^{M-1} d[n]d[n-l] \\ &+ \frac 1 M \sum_{n=0}^{M-1} x[n]d[n-l] + \frac 1 M \sum_{n=0}^{M-1} d[n]x[n-l]\\
              &= \underbrace{r_{xx}[l]}_{\mbox{period } N} + \underbrace{r_{dd}[l]}_{\delta[n]} + \underbrace{r_{xd}[l]}_{\simeq 0} + \underbrace{r_{dx}[l]}_{\simeq 0}.
\end{align*}

In the last equation $r_{xx}[l]$ is a periodic sequence having period $N$, as the autocorrelation of a periodic signal is periodic as well, possessing the same period. The latter term will possess peaks at multiples of the period $l=0, N, 2N,\dots$, maxima that can be easily detected. Autocorrelation sequence will have the same amplitudes as $l$ approaches $M$. The other terms deserve to be tackled separately,
\begin{itemize}
    \item $r_{dd}[l]$---This term is the unit impulse $\delta[n]$, as the random additive noise has an autocorrelation of $1$ located in the origin---purely random noises are never correlated with themselves, except in the origin, in the case of zero delay $l=0$;
    \item $r_{xd}[l]$---This term is roughly equal to zero, as $x[n]$ and $d[n]$ are scarcely correlated ($d[n]$ appears to be a squi\-si\-te\-ly random sequence);
    \item $r_{dx}[l]$---The same as above goes for the inverted cross-correlation between $d$ and $x$.
\end{itemize}

Indeed, evaluating autocorrelation of $w[n]$ is equivalent to performing two autocorrelations and two cross-correlations, with the value of the latter ones almost equal to zero and the autocorrelation of the random additive noise being simply a delta-impulse located in the origin, affecting only evaluation of time instant $0$ in the autocorrelation of $w[n]$. Peaks of $r_{ww}[l]$ for $l > 0$ are essentially due to the peaks of $r_{xx}[l]$ and can thus be employed to period detection as their peaks will essentially be the peaks related to the period of the autocorrelation of sequence $x[n]$, and can be soon identified as the period---on contrary, if autocorrelation of sequence $w[n]$ does not manifest any periodicity in the resulting peaks, there will likely be no period at all.

